---
title: "Session 4. Exploratory data analysis II: Visualization techniques"
author:
- name: Antonio Paez
  # Enter your name here:
- name: My Name
subject: "Workshop: Exploratory Data Analysis in `R`"

# The next two sections are for your own benefit. In the highlights you can briefly reflect about your learning experience. After completing the session, use this space to write your thoughts: what did you learn working on this session? What was easy? What was challenging? How were you challenged? What did you do that worked? What would you do differently? You can use more than one paragraph but remember to indent the paragraphs. This summary does not need to be very long, try to write it in about 200 words.
highlights: |
    This is my mini-reflection. Paragraphs must be indented.
    
    It can contain multiple paragraphs.
    
# Write the concepts that in your opinion are threshold concepts in this exercise. A threshold concept is a key idea that once you grasp it, it changes your understanding of a topic, phenomenon, subject, method, etc. Write between three and five threshold concepts that apply to your learning experience working on this exercise.
threshold_concepts: 
- threshold concept 1
- threshold concept 2 
- threshold concept 3
- threshold concept 4

# Do not edit below this line unless you know what you are doing
# --------------------------------------------------------------
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    # The project-template-default.tex file was heavily  adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: exercise-template-default.tex
font-family: Times New Roman    
always_allow_html: true
---

> "The value of experience is not in seeing much, but in seeing wisely."  
>
> --â€• William Osler

# Session outline

- Why visualization?
- {ggplot2}: a grammar of plots
- Creating empty plots
- Mapping data to aesthetics
- Geometric objects
- Univariate description
- Bivariate description
- Multivariate description

# Reminder

Exploratory Data Analysis is like detective work: we are interested in the fundamental characteristics of the data, like their central tendency, spread, and association, and we try to approach the data with as few assumptions as possible.

# Preliminaries

Clear the workspace from _all_ objects:
```{r}
rm(list = ls())
```

Load packages. Remember, packages are units of shareable code that augment the functionality of base `R`. For this session, the following package/s is/are used:
```{r}
library(corrr) # Correlations in R
library(dplyr) # A Grammar of Data Manipulation
library(edashop) # A Package for a Workshop on Exploratory Data Analysis
library(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics
library(janitor) # Simple Tools for Examining and Cleaning Dirty Data
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax
library(skimr) # Compact and Flexible Summaries of Data
library(tidyr) # Tidy Messy Data
```

We will also load the following data frames for this session:
```{r}
data("auctions_amf")
data("auctions_pf")
data("auctions_phy")
data("auctions_sef")
```

These are the same data frames that we used in Session 3, which for convenience we will join into a single table:
```{r}
auctions <- auctions_amf |>
  left_join(auctions_pf,
            by = "id") |>
  left_join(auctions_phy,
            by = "id") |>
  left_join(auctions_sef,
            by = "id")
```

# Why visualization?

In Session 3 of the workshop we discussed the use of summary statistics for exploring data. Summary statistics are data reduction techniques that focus on one, or possibly a small number of characteristics of the data. They are usually a single number that aims to describe the data from the perspective of the characteristic(s) of interest: for instance, their central tendency or their spread.

Summary statistics are very important, but as with any data reduction techniques, they are _insufficient_ and do not convey other aspects of the data. This is by design. Remember that the aim of EDA is to help us understand the data with our available cognitive capabilities, while avoiding overload. This is why summaries are so useful: they allow us to see, not a lot but wisely.

A complementary approach to summary statistics is the use of visualization techniques. Statistical plots exploit the wonderful ability of the human brain to process information visually. The cognitive power of brains to process alpha-numerical information is limited by our ability to retain information in short-term memory. How many number items can you remember while reading the following table and trying to distinguish patterns?
<!--- Function `slice_head()` filters rows based on position, beginning at the top of the table --->
```{r}
auctions_amf |>
  select(discount, premium) |>
  slice_head(n = 10)
```

What is the central tendency of `discount` taking into account only the numbers shown? Is the `discount` more or less spread than the `premium`? These properties of the data are not readily evident from a quick visual scan of the numbers. Summary statistics retrieve the desired information for us by "flattening" the data:
```{r}
auctions_amf |>
  select(discount, premium) |>
  slice_head(n = 10) |>
  summary()
```

To complicate matters, this is only a small part of the full table (only ten rows and six columns). The task of identifying patterns becomes increasingly complex as the number of observations and the number of variables grow.

Visualization techniques _encode_ the data in a way that makes fuller use of our visual data processing powers. Last session we introduced a simple visualization technique, called [stem-and-leaf](https://en.wikipedia.org/wiki/Stem-and-leaf_display) tables. The following stem-and-leaf tables present the _exact same information_ as that seen above, but reorganized in a way that engages our ability to process information visually: 

stem        | leaf
------------|--------
-0.1        |5
-0.2        |1
-0.3        |0
-0.4        |18
-0.5        |237
-0.6        |2
-0.7        |
-0.8        |1

stem        | leaf
------------|--------
-0.4        |0
-0.3        |
-0.2        |3
-0.1        |9
0.0         |24479
0.1         |3
0.2         |3

Stem-and-leaf tables encode one aspect of the data (frequency of values) in the form of _lengths_. We organize the data in such a way that the most common values appear as _long_ sequences of numbers, and the least common as _short_ sequences of numbers. Length is only one possible way of encoding aspects of data. Suppose that we wished to encode another aspect of the data, say, their _valence_. We could use colors to do this: red for negative values, and blue for non-negative values. This is shown in the following (modified) stem-and-leaf table:

stem        | leaf
------------|--------
-0.4        |\color{red}{0}
-0.3        |
-0.2        |\color{red}{3}
-0.1        |\color{red}{9}
0.0         |\color{blue}{24479}
0.1         |\color{blue}{3}
0.2         |\color{blue}{3}

The power of visualization techniques is that we can process multiple information channels _in parallel_. Shapes and colors are only two ways to encode statistical information; in addition, we can distinguish shapes, angles, areas, and positions. These encodings allow the brain to make immediate sense of the underlying values, in the blink of an eye (see Franconeri et al. [2021](https://journals.sagepub.com/doi/abs/10.1177/15291006211051956)), although with less precision than with summary statistics.

To better appreciate this power, consider the matrix of correlations of Session 3:
```{r}
auctions_amf |>
  select(where(is.numeric)) |>
  correlate(method = "pearson",
            use = "pairwise.complete.obs") |> 
  shave()
```

Now compare to 
```{r}
auctions_amf |>
  select(where(is.numeric)) |>
  correlate(method = "pearson",
            use = "pairwise.complete.obs") |> 
  shave() |>
  rplot()
```

By encoding valence using colors and magnitude using size, we can present the same information in a form that we naturally process with greater ease. Now compare to:
<!--- Function `network_plot()` creates a plot that maps the relationships --->
```{r}
auctions_amf |>
  select(where(is.numeric)) |>
  correlate(method = "pearson",
            use = "pairwise.complete.obs") |> 
  network_plot()
```

Again, same information, but easier to see how the variables correlate with each other. This plot encodes relationships with colored lines that vary in width and color, and it only requires that we call function `rplot()`. This convenience, on the other hand, comes at a price: what if we wanted to change the _size_ of the text? Or the _position_ of the legend? Or the _width_ of the lines? Or the rate at which the _color_ of the lines fades? By hard-coding a specific type of graph, `rplot()` is easy to use but relatively inflexible. A more flexible approach would generalize the common aspects of creating graphs so that they can be applied in a consistent but flexible fashion.

Before proceeding, pause for a moment and think about statistical visualization techniques that you might already be familiar. How do they encode data in visual form?

-
-
-

As a side note, if you are interested in learning more about how the brain processes visual information, Michael Friendly has some fantastic online resources about the [psychology of data visualization](https://friendly.github.io/6135/index.html).

# {ggplot2}: A grammar of plots

Just like the grammar of data manipulation discussed in Session 3 of the workshop was a convenient way to think about data operations, creating statistical plots is probably more flexible and natural when presented in the form of a grammar. A grammar of graphics was formalized by statistician and computer scientist [Leland Wilkinson](https://en.wikipedia.org/wiki/Leland_Wilkinson) in his book [A Grammar of Graphics](https://www.google.it/books/edition/The_Grammar_of_Graphics/ZiwLCAAAQBAJ). This book is the direct inspiration for package {[ggplot2](https://ggplot2.tidyverse.org/index.html)}, one of the most versatile and intuitive plotting packages around (see Wickham, [2010](https://doi.org/10.1198/jcgs.2009.07098))

Package {ggplot2} (the name stands for "grammar of graphics in 2-dimensions) implements a _layered_ grammar of graphics. Each function acts as an element in a phrase of visualization, and by adding them together we can create fully developed phrases of graphics. Unlike easy to implement functions ()

## A grammar of data visualization

## The anatomy of a plot

A plot is a two-dimensional surface on paper or a screen where we render information. What are the parts of a plot?

-
-
-

# The basic structure of a {ggplot2} phrase

- Create a {ggplot2} object: this is similar to creating a blank canvas for a statistical plot
- Populate the canvas with data (optional)
- Map data attributes to aesthetics on the plot (optional)
- Add geometric objects to the canvas (add data if you have not done so, and map attributes to aesthetics)
- Adjust the look of the plot

We will review each of these elements of the grammar next.

# Creating empty plots

In this step we initialize a {ggplot2} object for our desired statistical plot. At its simplest, it only requires that we state that we are creating a canvas:
```{r}
ggplot()
```

As we can see, this creates a blank plot for which we can state a number of different things. It is important to note that {ggplot2} objects can be named, and are not actually rendered until stated. For example, here we create a named {ggplot2} object that we will call very imaginatively `my_plot`:
```{r}
my_plot <- ggplot()
```

The canvas is not rendered until we call the object:
```{r}
my_plot
```

A blank canvas is not a statistical plot. For a statistical plot we need data. We can populate the object with a data frame at the time that we create the canvas; the data frame used in the function `ggplot()` is assumed to be the default for any geometric objects that we add to the plot later on. This step is optional, but if we are working with a single data frame it is an efficient way of declaring the data to be used for the plot:
```{r}
ggplot(data = auctions)
```

Notice that the canvas is still empty. But the underlying object already knows what data to use when required to do so.

# Mapping data to aesthetics

Before we can actually plot something on our blank canvas, we need to decide how the data will be represented graphically. What are the elements of 


# Geometric objects


# Univariate description


# Bivariate description


# Multivariate description



# Practice

1. Join tables `auctions_amf`, `auctions_pf`, `auctions_phy`, and `auctions_sef`.

2. Imagine that you are interested in the discount in distressed property sales. The discount (check `?auction_amf`) is the percent variation between the initial listed price (market value as apprised) and selling price. Describe and discuss this variable.

3. How does the discount relate to other variables in the data set? Discuss.

4. Propose some hypotheses about discount based on your exploration of the data set. How would you propose to investigate these hypotheses?

5. Imagine now that you are interested in the state of maintenance of properties that are sold in distressed conditions (check `?auction_phy`). Repeat questions 3 and 4 but for this variable.

6. What can you say so far about the relationship between discount and the categorical variables in the data set? Or between state of maintenance and the quantitative variables in the data set? Propose an approach to explore a combination of categorical and quantitative variables.


